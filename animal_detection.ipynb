{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d672531",
   "metadata": {},
   "source": [
    "# Animal detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1446c62",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e656b071",
   "metadata": {},
   "source": [
    "### Imports and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2b357c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Python libraries:\n",
    "import datetime\n",
    "import operator\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "from typing import List, Tuple\n",
    "import warnings\n",
    "\n",
    "# Data processing and modeling:\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.layers import Dense, Conv2D, Flatten, MaxPooling2D #, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.image import load_img, img_to_array, ImageDataGenerator\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf\n",
    "\n",
    "# Data visualization:\n",
    "from IPython import display\n",
    "# import matplotlib.pyplot as plt\n",
    "# from PIL import Image\n",
    "\n",
    "# Settings:\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3602860",
   "metadata": {},
   "source": [
    "### Data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aeb060e",
   "metadata": {},
   "source": [
    "To get a feel for the data we are working with, let us inspect two random images: one from the training set and one from the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a46ae909",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"data/\"\n",
    "# example_path = data_dir + \"train/Butterfly/0ac0ebf6b74941a5.jpg\"\n",
    "# example_img = load_img(example_path)\n",
    "# print(\"Random train image:\\n\")\n",
    "# print(\"Format:\\t\", example_img.format)\n",
    "# print(\"Mode:\\t\", example_img.mode)\n",
    "# print(\"Size:\\t\", example_img.size)\n",
    "# display.Image(example_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1323ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example_path = data_dir + \"test/Kangaroo/1df62fe536aae753.jpg\"\n",
    "# example_img = load_img(example_path)\n",
    "# print(\"Random test image:\\n\")\n",
    "# print(\"Format:\\t\", example_img.format)\n",
    "# print(\"Mode:\\t\", example_img.mode)\n",
    "# print(\"Size:\\t\", example_img.size)\n",
    "# display.Image(example_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2edd893",
   "metadata": {},
   "source": [
    "Both images have three color channels and they have almost the same resolution. For our CNN we need a fixed resolution, which might lead to rescaling or dropping of certain images. In the following we will inspect all image modes and resolutions to map the characteristics of our datasets. To this end, we will define a function that can obtain the full paths of all images in a given folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b959c493",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files(\n",
    "    dir_path: str, \n",
    "    extensions: Tuple[str] = (\".jpg\", \".jpeg\", \".png\"),\n",
    ") -> List[str]:\n",
    "    \"\"\"Obtain all files in the directory tree of the given folder.\n",
    "\n",
    "    Args:\n",
    "        dir_path: The path to the directory.\n",
    "        extensions: Optional argument that can be used to specify \n",
    "            desired extensions. By default only images are listed.\n",
    "\n",
    "    Returns:\n",
    "        List of full paths to all files that are contained in\n",
    "        the input directory, including subdirectories.\n",
    "    \"\"\"\n",
    "\n",
    "    list_of_items = os.listdir(dir_path)\n",
    "    all_files = []\n",
    "    for item in list_of_items:\n",
    "        full_path = os.path.join(dir_path, item).strip()\n",
    "        if os.path.isdir(full_path):\n",
    "            all_files = all_files + get_files(full_path)\n",
    "        else:\n",
    "            if full_path.endswith(extensions):\n",
    "                all_files.append(full_path)\n",
    "\n",
    "    return all_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c61c96c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 0 corrupt images in the dataset. Based on the remaining 1437 images, the dataset characteristics are as follows:\n",
      "\n",
      "Formats:\t [('JPEG', 1413), (None, 24)]\n",
      "\n",
      "Modes:\t\t [('RGB', 1437)]\n",
      "\n",
      "Sizes:\t\t [((1024, 768), 326), ((1024, 683), 240), ((683, 1024), 63), ((768, 1024), 55), ((1024, 685), 52), ((1024, 682), 49), ((1024, 681), 39), ((1024, 1024), 37), ((1024, 680), 33), ((1024, 576), 29), ((1024, 678), 20), ((1024, 640), 15), ((1024, 686), 11), ((1024, 684), 10), ((1024, 819), 8), ((682, 1024), 8), ((685, 1024), 8), ((681, 1024), 7), ((1024, 575), 6), ((1024, 737), 6), ((680, 1024), 5), ((1024, 679), 5), ((1024, 771), 5), ((1024, 668), 5), ((1024, 731), 5), ((1024, 766), 4), ((1024, 687), 4), ((768, 768), 4), ((1024, 716), 3), ((678, 1024), 3), ((1024, 620), 3), ((1024, 773), 3), ((1024, 767), 3), ((1024, 732), 3), ((819, 1024), 3), ((1024, 769), 3), ((687, 1024), 3), ((732, 1024), 3), ((1024, 725), 3), ((1024, 667), 3), ((1024, 692), 3), ((1024, 671), 3), ((912, 1024), 3), ((1024, 765), 3), ((1024, 573), 3), ((1024, 691), 2), ((1024, 503), 2), ((1024, 484), 2), ((1024, 648), 2), ((1024, 662), 2), ((1024, 735), 2), ((1024, 758), 2), ((1024, 899), 2), ((713, 1024), 2), ((1024, 818), 2), ((1024, 721), 2), ((1024, 796), 2), ((1024, 677), 2), ((1024, 761), 2), ((679, 1024), 2), ((1024, 658), 2), ((599, 1024), 2), ((1024, 789), 2), ((1024, 776), 2), ((1024, 813), 2), ((1024, 728), 2), ((1024, 800), 2), ((1024, 632), 2), ((1024, 770), 2), ((768, 962), 2), ((769, 1024), 2), ((1024, 726), 2), ((1005, 1024), 2), ((577, 1024), 2), ((1024, 816), 2), ((1024, 798), 2), ((1024, 1021), 2), ((1024, 638), 2), ((750, 1024), 2), ((1024, 630), 2), ((1024, 713), 2), ((796, 1024), 2), ((1024, 738), 2), ((1024, 853), 2), ((1024, 972), 2), ((1024, 664), 2), ((1024, 1011), 2), ((731, 1024), 2), ((1024, 574), 2), ((1024, 531), 2), ((791, 1024), 1), ((1024, 522), 1), ((1024, 790), 1), ((686, 1024), 1), ((702, 1024), 1), ((1024, 1020), 1), ((1024, 880), 1), ((1024, 781), 1), ((695, 1024), 1), ((974, 768), 1), ((1024, 724), 1), ((1024, 803), 1), ((1024, 814), 1), ((1024, 1014), 1), ((768, 788), 1), ((1024, 690), 1), ((920, 1024), 1), ((1024, 541), 1), ((1024, 521), 1), ((955, 768), 1), ((1024, 862), 1), ((1024, 785), 1), ((1024, 999), 1), ((1024, 861), 1), ((768, 897), 1), ((1024, 665), 1), ((997, 1024), 1), ((1024, 542), 1), ((539, 1024), 1), ((1024, 833), 1), ((1024, 777), 1), ((823, 768), 1), ((1024, 669), 1), ((1024, 701), 1), ((771, 1024), 1), ((1024, 646), 1), ((1024, 599), 1), ((1024, 670), 1), ((1023, 1024), 1), ((1024, 502), 1), ((772, 1024), 1), ((895, 1024), 1), ((1024, 529), 1), ((1024, 747), 1), ((1024, 714), 1), ((1024, 850), 1), ((1024, 534), 1), ((1024, 907), 1), ((897, 1024), 1), ((1024, 886), 1), ((894, 1024), 1), ((1024, 644), 1), ((1024, 577), 1), ((1024, 693), 1), ((1024, 597), 1), ((1024, 877), 1), ((1024, 882), 1), ((1024, 879), 1), ((756, 1024), 1), ((1024, 628), 1), ((1024, 642), 1), ((1024, 1001), 1), ((1024, 848), 1), ((986, 768), 1), ((1024, 956), 1), ((1024, 831), 1), ((706, 1024), 1), ((1024, 595), 1), ((631, 1024), 1), ((1024, 629), 1), ((710, 1024), 1), ((1024, 448), 1), ((1024, 885), 1), ((437, 1024), 1), ((863, 1024), 1), ((915, 768), 1), ((1024, 442), 1), ((911, 1024), 1), ((564, 1024), 1), ((936, 1024), 1), ((1024, 547), 1), ((673, 1024), 1), ((523, 1024), 1), ((1024, 913), 1), ((1024, 673), 1), ((1024, 742), 1), ((1024, 619), 1), ((812, 1024), 1), ((1024, 917), 1), ((1024, 579), 1), ((624, 1024), 1), ((1024, 562), 1), ((817, 1024), 1), ((1024, 805), 1), ((1024, 786), 1), ((1024, 949), 1), ((1024, 828), 1), ((1024, 666), 1), ((1024, 581), 1), ((1024, 774), 1), ((905, 1024), 1), ((1024, 820), 1), ((1024, 952), 1), ((1024, 566), 1), ((1024, 1003), 1), ((1024, 878), 1), ((1024, 830), 1), ((893, 768), 1), ((829, 1024), 1), ((1024, 838), 1), ((1024, 553), 1), ((1024, 1007), 1), ((932, 1024), 1), ((1024, 634), 1), ((767, 1024), 1), ((1024, 530), 1), ((1024, 730), 1), ((807, 768), 1), ((1024, 970), 1), ((1003, 1024), 1), ((1024, 826), 1), ((915, 1024), 1), ((952, 1024), 1), ((736, 1024), 1), ((994, 768), 1), ((663, 1024), 1), ((568, 1024), 1), ((761, 1024), 1), ((1024, 651), 1), ((633, 1024), 1), ((622, 1024), 1), ((855, 1024), 1), ((1024, 905), 1), ((670, 1024), 1), ((1024, 817), 1), ((470, 1024), 1), ((1024, 808), 1), ((836, 1024), 1), ((1024, 348), 1), ((1004, 1024), 1), ((1024, 402), 1), ((720, 1024), 1), ((1024, 536), 1), ((1024, 706), 1), ((1024, 873), 1), ((1024, 370), 1), ((1024, 974), 1), ((1024, 695), 1), ((1024, 755), 1), ((726, 1024), 1), ((1024, 868), 1), ((749, 1024), 1), ((1024, 901), 1), ((1024, 672), 1), ((1021, 1024), 1), ((1024, 449), 1), ((1024, 744), 1), ((976, 1024), 1), ((1024, 723), 1), ((763, 1024), 1), ((1024, 912), 1), ((1024, 612), 1), ((900, 1024), 1), ((1024, 922), 1), ((1024, 417), 1), ((1024, 839), 1), ((985, 1024), 1), ((970, 768), 1), ((1024, 704), 1), ((1024, 757), 1), ((1024, 660), 1), ((738, 1024), 1), ((1024, 718), 1), ((1024, 467), 1), ((965, 1024), 1), ((1024, 710), 1), ((776, 1024), 1), ((925, 768), 1), ((1024, 778), 1), ((1024, 394), 1), ((751, 1024), 1), ((1024, 891), 1), ((820, 1024), 1), ((1024, 643), 1), ((917, 1024), 1), ((1024, 587), 1), ((950, 1024), 1), ((1017, 768), 1), ((656, 1024), 1), ((839, 768), 1), ((1024, 676), 1), ((645, 1024), 1), ((1024, 645), 1), ((768, 851), 1), ((789, 1024), 1), ((768, 876), 1), ((1024, 958), 1), ((1024, 356), 1), ((855, 768), 1), ((1024, 650), 1), ((1024, 699), 1), ((1024, 860), 1), ((1024, 793), 1), ((1024, 897), 1), ((1024, 762), 1), ((1024, 849), 1), ((886, 1024), 1), ((677, 1024), 1), ((1024, 753), 1), ((1024, 604), 1), ((1024, 578), 1), ((1024, 399), 1), ((1024, 568), 1), ((1024, 572), 1), ((1024, 752), 1), ((1024, 697), 1), ((1024, 605), 1), ((1024, 513), 1), ((1024, 745), 1), ((874, 1024), 1), ((1024, 733), 1), ((1024, 858), 1), ((1024, 829), 1), ((1024, 583), 1), ((744, 1024), 1), ((1024, 621), 1), ((1024, 743), 1), ((1024, 494), 1), ((784, 1024), 1), ((1024, 656), 1), ((1024, 649), 1), ((755, 1024), 1), ((1024, 487), 1), ((1024, 565), 1), ((1024, 596), 1), ((834, 1024), 1), ((1024, 434), 1)]\n",
      "\n",
      "Resolutions:\t [(1048576, 37), (1047552, 1), (1045504, 3), (1044480, 1), (1038336, 1), (1035264, 2), (1031168, 1), (1029120, 2), (1028096, 1), (1027072, 2), (1025024, 1), (1022976, 1), (1020928, 1), (1008640, 1), (999424, 1), (997376, 1), (995328, 2), (993280, 1), (988160, 1), (980992, 1), (978944, 1), (974848, 2), (972800, 1), (971776, 1), (958464, 1), (954368, 1), (944128, 1), (942080, 1), (939008, 2), (936960, 1), (934912, 1), (933888, 4), (932864, 1), (928768, 1), (926720, 2), (922624, 1), (921600, 1), (920576, 2), (918528, 2), (916480, 1), (915456, 1), (912384, 1), (907264, 2), (906240, 1), (903168, 1), (901120, 1), (900096, 1), (899072, 1), (898048, 1), (894976, 1), (893952, 1), (888832, 1), (883712, 1), (882688, 1), (881664, 1), (880640, 1), (878592, 1), (875520, 1), (873472, 2), (870400, 1), (869376, 1), (868352, 1), (859136, 1), (858112, 1), (856064, 1), (854016, 1), (852992, 1), (850944, 1), (849920, 1), (848896, 2), (847872, 1), (845824, 1), (839680, 2), (838656, 11), (837632, 2), (836608, 2), (835584, 2), (833536, 1), (832512, 2), (831488, 1), (827392, 1), (824320, 1), (822272, 1), (819200, 2), (817152, 2), (815104, 4), (812032, 1), (809984, 1), (808960, 1), (807936, 3), (804864, 1), (803840, 1), (802816, 1), (799744, 1), (796672, 1), (795648, 1), (794624, 3), (792576, 1), (791552, 3), (790528, 1), (789504, 6), (788480, 2), (787456, 5), (786432, 381), (785408, 4), (784384, 4), (783360, 3), (781312, 1), (781056, 1), (780288, 1), (779264, 3), (776192, 2), (775168, 1), (774144, 1), (773120, 2), (771072, 1), (770048, 1), (769024, 1), (768000, 2), (766976, 1), (764928, 1), (763392, 1), (762880, 1), (761856, 2), (760832, 1), (759808, 1), (757248, 1), (755712, 3), (754688, 6), (753664, 1), (752640, 2), (750592, 1), (749568, 6), (748544, 7), (748032, 1), (747520, 1), (745472, 2), (744960, 1), (743424, 3), (742400, 3), (741376, 1), (740352, 1), (738816, 2), (738304, 2), (737280, 1), (735232, 1), (733440, 1), (733184, 3), (731136, 1), (730112, 4), (727040, 2), (722944, 2), (720896, 1), (718848, 1), (717824, 1), (715776, 1), (713728, 1), (711680, 2), (710400, 1), (709632, 1), (708608, 3), (707584, 2), (706560, 1), (703488, 7), (702720, 1), (702464, 12), (701440, 60), (700416, 10), (699392, 303), (698368, 57), (697344, 46), (696320, 38), (695296, 7), (694272, 23), (693248, 3), (692224, 1), (689152, 2), (688896, 1), (688128, 1), (687104, 3), (686080, 2), (685824, 1), (685056, 1), (684032, 5), (683008, 3), (681984, 1), (680960, 1), (679936, 2), (678912, 1), (677888, 2), (675840, 1), (673792, 2), (672768, 1), (671744, 2), (666624, 1), (665600, 1), (664576, 1), (663552, 2), (661504, 1), (660480, 2), (659456, 1), (658432, 1), (657408, 1), (656640, 1), (655360, 15), (653568, 1), (653312, 2), (649216, 1), (648192, 1), (647168, 2), (646144, 1), (645120, 2), (644352, 1), (644096, 1), (643072, 1), (638976, 1), (636928, 1), (635904, 1), (634880, 3), (633856, 1), (632064, 1), (626688, 1), (619776, 1), (619520, 1), (618496, 1), (613376, 3), (611328, 1), (610304, 1), (609280, 1), (605184, 1), (601088, 1), (596992, 1), (594944, 1), (592896, 1), (591872, 1), (590848, 3), (589824, 33), (588800, 6), (587776, 2), (586752, 3), (585728, 1), (581632, 2), (579584, 1), (578560, 1), (577536, 1), (575488, 1), (566272, 1), (560128, 1), (555008, 1), (553984, 1), (551936, 1), (548864, 1), (546816, 1), (543744, 2), (542720, 1), (541696, 1), (535552, 1), (534528, 1), (533504, 1), (525312, 1), (515072, 2), (514048, 1), (505856, 1), (498688, 1), (495616, 2), (481280, 1), (478208, 1), (459776, 1), (458752, 1), (452608, 1), (447488, 1), (444416, 1), (427008, 1), (411648, 1), (408576, 1), (403456, 1), (378880, 1), (364544, 1), (356352, 1)]\n"
     ]
    }
   ],
   "source": [
    "img_paths = get_files(data_dir + \"train\")\n",
    "data_info = {\"Formats\": {}, \"Modes\": {}, \"Sizes\": {}, \"Resolutions\": {}}\n",
    "failed_imgs = []\n",
    "successes = 0\n",
    "\n",
    "# Determine characterics for each image:\n",
    "for img_path in img_paths:\n",
    "    try:\n",
    "        img = load_img(img_path)      \n",
    "        ext = img.format\n",
    "        if ext not in data_info[\"Formats\"]:\n",
    "            data_info[\"Formats\"][ext] = 1\n",
    "        else:\n",
    "            data_info[\"Formats\"][ext] += 1\n",
    "            \n",
    "        mode = img.mode\n",
    "        if mode not in data_info[\"Modes\"]:\n",
    "            data_info[\"Modes\"][mode] = 1\n",
    "        else:\n",
    "            data_info[\"Modes\"][mode] += 1\n",
    "            \n",
    "        size = img.size\n",
    "        if size not in data_info[\"Sizes\"]:\n",
    "            data_info[\"Sizes\"][size] = 1\n",
    "        else:\n",
    "            data_info[\"Sizes\"][size] += 1\n",
    "            \n",
    "        resolution = size[0] * size[1]\n",
    "        if resolution not in data_info[\"Resolutions\"]:\n",
    "            data_info[\"Resolutions\"][resolution] = 1\n",
    "        else:\n",
    "            data_info[\"Resolutions\"][resolution] += 1\n",
    "        \n",
    "        successes += 1\n",
    "    except:\n",
    "        failed_imgs.append(img_path)\n",
    "\n",
    "# Sort dictionaries by values:\n",
    "data_info[\"Formats\"] = sorted(data_info[\"Formats\"].items(), key=operator.itemgetter(1), reverse=True)\n",
    "data_info[\"Modes\"] = sorted(data_info[\"Modes\"].items(), key=operator.itemgetter(1), reverse=True)\n",
    "data_info[\"Sizes\"] = sorted(data_info[\"Sizes\"].items(), key=operator.itemgetter(1), reverse=True)\n",
    "data_info[\"Resolutions\"] = sorted(data_info[\"Resolutions\"].items(), key=operator.itemgetter(0), reverse=True)\n",
    "\n",
    "print(f\"There are {len(failed_imgs)} corrupt images in the dataset. Based on the remaining {successes} images, the dataset characteristics are as follows:\")\n",
    "print(\"\\nFormats:\\t\", data_info[\"Formats\"])\n",
    "print(\"\\nModes:\\t\\t\", data_info[\"Modes\"])\n",
    "print(\"\\nSizes:\\t\\t\", data_info[\"Sizes\"])\n",
    "print(\"\\nResolutions:\\t\", data_info[\"Resolutions\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12682a89",
   "metadata": {},
   "source": [
    "All images in the dataset are of the RGB type. The sizes, however, vary considerably. The most common number of pixels for the long edge is 1024. The smallest resolutions found in the dataset are ~300,000 pixels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426d4d5e",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a38b30",
   "metadata": {},
   "source": [
    "Before we can train a model, we need to put our data into the right form. Our CNN will need a fixed input size. In other words, all input images require the same resolution. It is convenient to work with a square resolution so that horizontal and vertical images are treated on an equal footing. To compensate for the original non-squareness we can pad the images with zeros (i.e. add black pixels). Given the smallest resolutions found in the dataset, a good resolution to work with would be 512 x 512 = 262,144 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ed9ec52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of tensor for input features:\t (1437, 512, 512, 3)\n",
      "Dimensions of tensor for output labels:\t\t (1437, 4)\n"
     ]
    }
   ],
   "source": [
    "# Set desired image dimensions:\n",
    "target_height = 512\n",
    "target_width = 512\n",
    "\n",
    "# Resize images to target dimensions with padding.\n",
    "train_dir = data_dir + \"train\"\n",
    "img_paths = get_files(train_dir)\n",
    "img_list = []\n",
    "label_list = []\n",
    "for img_path in img_paths:\n",
    "    img = load_img(img_path)\n",
    "    img_arr = img_to_array(img)\n",
    "    img_pad_arr = tf.image.resize_with_pad(\n",
    "        img_arr,\n",
    "        target_height,\n",
    "        target_width,\n",
    "    )\n",
    "    img_list.append(img_pad_arr)\n",
    "    label = os.path.basename(os.path.dirname(img_path))\n",
    "    label_list.append(label)\n",
    "\n",
    "# Create feature and label tensors for training data:    \n",
    "train_features = np.stack(img_list, axis=0)\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels = label_encoder.fit_transform(label_list)\n",
    "train_labels = to_categorical(train_labels)\n",
    "\n",
    "print(\"Dimensions of tensor for input features:\\t\", train_features.shape)\n",
    "print(\"Dimensions of tensor for output labels:\\t\\t\", train_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6e48ce",
   "metadata": {},
   "source": [
    "### Model building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a48bf4",
   "metadata": {},
   "source": [
    "In this section we build and train a CNN using Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e6c989",
   "metadata": {},
   "source": [
    "The model architecture is given by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77f3f3ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"CNN_for_animal_classification\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 512, 512, 64)      4864      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 128, 128, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 128, 128, 32)      51232     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 16)        12816     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 16)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 100)               102500    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4)                 404       \n",
      "=================================================================\n",
      "Total params: 171,816\n",
      "Trainable params: 171,816\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "num_classes = train_labels.shape[1]\n",
    "\n",
    "model = Sequential(name=\"CNN_for_animal_classification\")\n",
    "model.add(Conv2D(64, kernel_size=(5, 5), padding=\"same\", activation=\"relu\", input_shape=(512, 512, 3)))\n",
    "model.add(MaxPooling2D(pool_size=(4, 4), padding=\"same\"))\n",
    "model.add(Conv2D(32, kernel_size=(5, 5), padding=\"same\", activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(4, 4), padding=\"same\"))\n",
    "model.add(Conv2D(16, kernel_size=(5, 5), padding=\"same\", activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(4, 4), padding=\"same\"))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100, activation=\"relu\"))\n",
    "model.add(Dense(num_classes, activation=\"softmax\"))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eecdb60",
   "metadata": {},
   "source": [
    "Next, we can set the loss function, the optimization algorithm, and the proper metric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "846ff99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=\"adam\", \n",
    "    loss=\"categorical_crossentropy\", \n",
    "    metrics=['accuracy'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a184116",
   "metadata": {},
   "source": [
    "The training and validation sets can be defined using the ImageDataGenerator. Since we only want to apply data augmentation to the training set, we need to set the random seeds in both generators equal to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9e5e468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters:\n",
    "val_frac = 0.2\n",
    "batch_size = 8\n",
    "\n",
    "# Specify training set, including data augmentation:\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=6,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    fill_mode=\"constant\",\n",
    "    cval=0,\n",
    "    horizontal_flip=True,\n",
    "    rescale=1./255,\n",
    "    validation_split=val_frac,\n",
    ")\n",
    "train_generator = train_datagen.flow(\n",
    "    train_features,\n",
    "    y=train_labels,\n",
    "    batch_size=batch_size,\n",
    "    seed=0,\n",
    "    subset=\"training\",\n",
    ")\n",
    "\n",
    "# Specify validation set:\n",
    "val_datagen = ImageDataGenerator(\n",
    "    rescale=1./255, \n",
    "    validation_split=val_frac,\n",
    ")\n",
    "val_generator = val_datagen.flow(\n",
    "    train_features,\n",
    "    y=train_labels,\n",
    "    batch_size=batch_size,\n",
    "    seed=0,\n",
    "    subset=\"validation\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40552162",
   "metadata": {},
   "source": [
    "During training, we will visualize the loss function using TensorBoard. To view current progress, use the refresh button."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b564c80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-f0db4dd35a8a4139\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-f0db4dd35a8a4139\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define TensorBoard callback:\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "if os.path.isdir(\"logs\"):\n",
    "    shutil.rmtree(\"logs\")\n",
    "log_dir = os.path.join(\"logs\", current_time)\n",
    "tensorboard_callback = TensorBoard(\n",
    "        log_dir, \n",
    "        histogram_freq=1, \n",
    "        update_freq=\"batch\",\n",
    ")\n",
    "\n",
    "# Visualize loss function:\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca37a20",
   "metadata": {},
   "source": [
    "Finally we can train our CNN. The model weights are saved to an h5 file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af92fd3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "  8/144 [>.............................] - ETA: 8:26 - loss: 1.3711 - accuracy: 0.1873"
     ]
    }
   ],
   "source": [
    "# Train model:\n",
    "epochs = 6\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    epochs=epochs,\n",
    "    validation_data=val_generator,\n",
    "    callbacks=[tensorboard_callback],\n",
    ")\n",
    "\n",
    "# Save weights:\n",
    "weights_path = \"weights_\" + current_time + \".h5\"\n",
    "model.save_weights(weights_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6d9aaf",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "07bf70ac",
   "metadata": {},
   "source": [
    "#### To do:\n",
    "\n",
    "**Example**: https://gist.github.com/fchollet/0830affa1f7f19fd47b06d4cf89ed44d\n",
    "\n",
    "**Try this for padding**: https://www.tensorflow.org/api_docs/python/tf/image/resize_with_pad or this: https://www.tensorflow.org/api_docs/python/tf/image/resize_with_crop_or_pad\n",
    "\n",
    "- Add pretrained weights.\n",
    "- Add data augmentation. Define a ImageDataGenerator and then use the flow_from_directory method.\n",
    "- Maybe apply data standardization.\n",
    "- Apply regularization (at least drop-out).\n",
    "- Maybe include batch normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dae2e78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39-animals",
   "language": "python",
   "name": "py39-animals"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
